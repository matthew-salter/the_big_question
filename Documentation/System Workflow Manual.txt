üîß SYSTEM WORKFLOW MANUAL

# Structured Report Generation Engine ‚Äì System Workflow Manual

## üìò README / Manual

This manual explains how the Structured Report Generation Engine works. It outlines the logic, design decisions, and workflows across the project ‚Äî acting as a plain-English reference guide to accompany the Master Architecture Document.

---

## üéØ Purpose

The engine transforms natural language questions into structured, multi-section reports using reusable AI prompt templates, JSON output formatting, and automation orchestration.

We are currently focused on a single high-value path:

> **Question Topic:** Commodity  
> **Question Type:** Predictive  
> **Report Type:** Predictive Report (weighted-factor forecasting logic)

---

## üß± Key Architecture Concepts

### 1. Modular Prompt Flow
Each stage in the system is handled by a distinct prompt (e.g. Prompt 1, Prompt 2A, 2B...).  
Each prompt is:
- Stateless (does not rely on memory)
- Context-driven (via mapped variables)
- JSON-structured in both input and output

### 2. Three-Layer Input Classification

| Layer            | Description                               | Example                       |
|------------------|--------------------------------------------|-------------------------------|
| **Question Topic** | What the question is about (domain)        | Commodity, Healthcare, Tech   |
| **Question Type**  | What the question is asking for (intent)  | Predictive, Comparative, etc. |
| **Report Type**    | What format is needed to answer it        | Predictive Report, etc.       |

---

## üß© Prompt Structure

Each prompt follows this logic:

- `INPUT:` Injected variables from Zapier or Python (e.g. `{{question}}`, `{{commodity}}`, `{{time_range}}`, etc.)
- `OBJECTIVE:` The task instruction for that stage (e.g. generate 10 sections)
- `OUTPUT:` Structured JSON, predictable, flat/nested depending on stage

Example injected variables:

{{question}}
{{target_variable}}
{{commodity}}
{{region}}
{{time_range}}
{{reference_age_range}}


---

## üß† Prompt Design Standards

### 1. Prompt Independence
No prompt can assume prior memory.  
All context must be passed explicitly.  
Each prompt must function as a fully self-contained unit.

### 2. Optional Blocks
Wrapped in logic like:
```python
if supporting_documents:
    prompt += f"...context block..."

Omit entire prompt sections if input is not provided.

3. Prediction Range Logic
Predictive prompts must output:
change_low / change_high (expected % change)
effect_low / effect_high (impact of that change)
makeup_percent (section influence on total outcome
This structure enables stable forecasting and summed weighted calculations.

4. Tolerance & Realism
Prompt outputs should follow domain-specific tolerance bands (e.g. no ¬±40% swings unless justified). This is enforced in Prompt 1 logic.

5. Source Aggregation (later prompts)
Multiple sources may be considered by the AI, but only one reference will be cited in the asset output. Source diversity improves answer stability.

---

üß† Prompt Sequence
| Stage     | Prompt        | Function                                                 |
|-----------|---------------|----------------------------------------------------------|
| Stage 1   | Prompt 1      | Classify topic, question type, extract context variables |
| Stage 2   | Prompt 2A     | Generate report structure: sections, sub-sections, weights |
| Stage 3   | Prompt 2B/2C  | Forecast changes and calculate effects per section       |
| Stage 4   | Prompt 3+     | Pull in references, chart data, captions, wrap-up assets |

---

üõ† Developer Practices
Always use {{variable}} format for clarity and injection

Validate every prompt output with a JSON linter

Separate prompt logic from routing logic (keep scripts modular)

Store prompt inputs and outputs as records for later reuse

Log all prompt results for rollback / debugging

---

üîÅ Regeneration + Feedback
Each section of a report is isolated by ID (e.g. forecast_3)

If feedback is flagged (e.g. ‚Äúsection too vague‚Äù), we:

Inject original inputs + feedback

Rerun just that section

Replace output in manifest ‚Üí reassemble full report

---

üß™ Current Focus Area
We are testing Predictive Reports on Commodity-topic questions. These follow this format:

‚ÄúWhat will the [target_variable] of [commodity] be in [region] over [time_range]?‚Äù

Examples:

What will the price of chicken be in the UK in the next 12 months?

How much will demand for cobalt grow globally over the next 3 years?

These questions are routed into a Predictive Report, which:

Identifies 10 key factors influencing the outcome

Assigns % makeup to each

Forecasts directional change

Calculates total weighted effect

---

üîÆ Future Use Cases
Prompt 7: Regenerate individual section post-human review

Prompt 8: Compare two time periods (Q2 vs Q3 delta)

Prompt 9: Add supporting document overlays (research, PDF)

Prompt 10+: Export chart-ready data and PDF design outputs

---

‚úÖ Final Note
This engine is not a ‚ÄúCommodity Report System‚Äù.
It is a Predictive Report Engine, currently being applied to commodity-topic questions.
Future domains (Finance, Health, Education) and other question types (Evaluative, Diagnostic, etc.) will follow the same architecture.

This document + the Master Architecture Document form your operational baseline.
